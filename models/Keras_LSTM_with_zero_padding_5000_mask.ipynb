{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the third attempt at training a NN. I am doing sequence padding\n",
    "# again, but truncating to as low a value as possible to make training\n",
    "# faster and avoid memory issues (I've been having crashes on the current)\n",
    "# feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.core import Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "malware_classes = [\"Agent\", \"AutoRun\", \"FraudLoad\", \"FraudPack\", \"Hupigon\", \"Krap\",\n",
    "           \"Lipler\", \"Magania\", \"None\", \"Poison\", \"Swizzor\", \"Tdss\",\n",
    "           \"VB\", \"Virut\", \"Zbot\"]\n",
    "\n",
    "# a function for writing predictions in the required format\n",
    "def write_predictions(predictions, ids, outfile):\n",
    "    \"\"\"\n",
    "    assumes len(predictions) == len(ids), and that predictions[i] is the\n",
    "    index of the predicted class with the malware_classes list above for\n",
    "    the executable corresponding to ids[i].\n",
    "    outfile will be overwritten\n",
    "    \"\"\"\n",
    "    with open(outfile,\"w+\") as f:\n",
    "        # write header\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i, history_id in enumerate(ids):\n",
    "            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))\n",
    "\n",
    "def classes_to_Y(classes):\n",
    "    output = []\n",
    "    for cls in classes:\n",
    "        output.append(malware_classes.index(cls))\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training classes\n",
    "classes = np.load(\"../data/features/train_classes.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  734   566     4     5  1150   558   740     4   785  5849    11     3\n",
      "     1   547    12    40   804  2049   946     1   947     1   811   113\n",
      "   805     5   803    68  3872   808  1827  2759   807  1591   809  1174\n",
      "   806   654  1590  1592   478   604  5653   495   844    11     3     1\n",
      "   547    10     4     6  1222    13  4798     7  1983    12    40    14\n",
      "    11     3     2     9   680     8    10     4     6   700    13   702\n",
      "     7   698    12   696    14    11     3     2     9   134     8    10\n",
      "     4     6   309    13   310     7   308    12   303    14    11     3\n",
      "     2   694   709     2   678   725     5   697   723   727    10     4\n",
      "     6   729    13   730     7   728    12    40    14    11     3     2\n",
      "     9   798     8    10     4     6   818    13   817     7   792    12\n",
      "   802    14    11     3     2     9   216     8    10     4     6   476\n",
      "    13   477     7   475    12   474    14    11     3     2     9   774\n",
      "     8    10     4     6   779    13   780     7   778    12   775    14\n",
      "    11     3     2     9   776     8    10     4     6   783    13   784\n",
      "     7   597    12   777    14    11     3     2     9   742     8    10\n",
      "     4     6   747    13   746     7   743    12   744    14    11     3\n",
      "     2     9   543     8    10     4     6   633    13   636     7   630\n",
      "    12   635    14    11     3     2     9   812     8    10     4     6\n",
      "   815    13   816     7   814    12   813    14    11     3     2     9\n",
      "   751     8    10     4     6   755    13   754     7   748    12   752\n",
      "    14    11     3     2     9   600     8    10     4     6   692    13\n",
      "   693     7   691    12   684    14    11     3     2     9   515     8\n",
      "    10     4     6   611    13   612     7   610    12   609    14    11\n",
      "     3     2     9   840     8    10     4     6   842    13   843     7\n",
      "   828    12   841    14    11     3     2     9   834     8    10     4\n",
      "     6   838    13   839     7   597    12   836  1104   145  1136    14\n",
      "    11     3     2     9   790     8    10     4     6   800    13   801\n",
      "     7   799    12   791   463    29    22    24    23    30   244   883\n",
      "     1   547    29    22    24    23    30   244   677    42    22    24\n",
      "    23    30   244   677    41   935   537   133   244  1265   618     4\n",
      "   291   152   532   549   551   509   468     5   537   133   244  1264\n",
      "   618     4   291   152   532   549   551   509   468     5   537   133\n",
      "   244  1258   618     4   291   152   532   549   551   509   468     5\n",
      "   537   133   244  1237   618     4   291   152   532   549   551   509\n",
      "   468     5   537   133   244  1263   618     4   291   152   532   549\n",
      "   551   509   468     5    29    22    64   624   576   638    42    22\n",
      "    64   624   576   638    41   845   749    42    22    64   624   576\n",
      "   638    41   576   749   463    29    22    24    23    30   244    42\n",
      "    22    24    23    30   244    41  1266   537   133   244  1260  1273\n",
      "     4   291   152   532   549   551  1261  1270     4   291   152   532\n",
      "   468     5   758   761  1132   555  5653   759  1168   760   757   758\n",
      "   761  1130   555  5653   759  1167   760   757    29    22    24    23\n",
      "    30     2   442    45  1103    42    22    24    23    30     2   442\n",
      "    45  1103    41   616    32    14    11     3     2     9   566     8\n",
      "    10     4     6   787    13   786     7   695    12   782   463    14\n",
      "    11     3     2     9   821   616    10     4     6 16495    13 16495\n",
      "     7     5    12   895   463    14    11     3     2     9   515     8\n",
      "    10     4     6   611    13   612     7   610    12   609    14    11\n",
      "     3     2     9   821   616    10     4     6  1095    13  1094     7\n",
      "   716    12   895   463   583   329 16568   115   616   171   590   616\n",
      "   580     5   579     5   575     5   578     5   489   756   585   613\n",
      "   589   586   587   584   588    29    22    64    23    30   244    42\n",
      "    22    64    23    30   244    41  1205   478  1230   632    29    22\n",
      "    24    23    30   244   677    42    22    24    23    30   244   677\n",
      "    41   935   463    14    11     3     2     9   821   616    10     4\n",
      "     6  1095    13  1094     7   716    12   895   583   329 17885   115\n",
      "   467   171  4342  4342  4342   580  5782   579 18236   575   108   578\n",
      "   354   489  1131  1152  1151   585   613  1577  1064   589   586 14554\n",
      "  1431  1625   587   584   588  1148    14    11     3     2     9   790\n",
      "     8    10     4     6   800    13   801     7   799    12   791    14\n",
      "    11     3     2     9   751     8    10     4     6   755    13   754\n",
      "     7   748    12   752   499    32   452   448   486    40   484  5378\n",
      "     1     1    41     1   499    32   452   448   486    40   484  5378\n",
      "     1     1    41     1   583   329  9611   115   821  1516   171   254\n",
      "   580     5   579     5   575     5   578     5   489   756   585   613\n",
      "   589   586   587   584   588   156   115  2209   256   583   329 11878\n",
      "   115   539   171 17247   580     5   579     5   575     5   578     5\n",
      "   489   756   585   613   589   586   587   584   588   614   329 17885\n",
      "  1251  1389     1   685   256   614   329 17885  1251  1389     1   685\n",
      "    29    22    64   624   576   638    42    22    64   624   576   638\n",
      "    41   845   749    42    22    64   624   576   638    41   576   749\n",
      "    29    22    64    23    30   244  1372    29    22    24    23    30\n",
      "   244  1372   537   133  1069   735  1640  6224   468     5   463   495\n",
      "   478   558  1150]\n"
     ]
    }
   ],
   "source": [
    "# load sparse matrix of training data\n",
    "full_features = np.load(\"../data/features/100_cutoff_alphabet_19679_word_to_intseq.npy\")\n",
    "# The way masking works, 0 must be maintained as a \"ignore this\" symbol,\n",
    "# so I have to increase the index by 1 to free the zero index.\n",
    "for i in xrange(len(full_features)):\n",
    "    full_features[i] +=1 #Add 1 to each of the arrays in the array\n",
    "    \n",
    "print full_features[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The way masking works, 0 must be maintained as a \"ignore this\" symbol,\n",
    "# so I have to increase the index by 1 to free the zero index.\n",
    "for i in xrange(len(full_features)):\n",
    "    full_features[i] +=1 #Add 1 to each of the arrays in the array\n",
    "    \n",
    "print full_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlengths = [len(x) for x in full_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 200000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt8AAAFpCAYAAABEcUQxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFRRJREFUeJzt3W+MZXd93/HPt97ERQRSu95alm26tuRGMkh1wsp1VRJR\nkWIDaey0FVpUFbdFuBVuFNRW1bpIDX1g1WmVVEUtRE5BmIpg3CYIS0BT40ZFfQBmjRz8B1wvYIRX\n/gdUdaRWbu18+2DOOtfLzu54d+Y7szuvl3Q1Z373njPn/nzm7tv3nju3ujsAAMDW+xPbvQMAALBb\niG8AABgivgEAYIj4BgCAIeIbAACGiG8AABgivgEAYIj4BgCAIeIbAACGiG8AABiyZ7t34GQuuOCC\n3rdv33bvBgAAZ7H777//+929d6t/zo6P73379uXQoUPbvRsAAJzFquq7Ez/HaScAADBEfAMAwBDx\nDQAAQ8Q3AAAMEd8AADBEfAMAwBDxDQAAQ8Q3AAAMEd8AADBEfAMAwBDxDQAAQ8Q3AAAMEd8AADBk\nz3bvwEbsO/i5Ldv247e9Y8u2DQAAqzzzDQAAQ8Q3AAAMEd8AADBEfAMAwBDxDQAAQ8Q3AAAMEd8A\nADBEfAMAwBDxDQAAQ8Q3AAAMEd8AADBEfAMAwBDxDQAAQ8Q3AAAMEd8AADBEfAMAwBDxDQAAQ/Zs\n9w5st30HP7dl2378tnds2bYBADjzeOYbAACGiG8AABgivgEAYIj4BgCAIeIbAACGiG8AABgivgEA\nYIj4BgCAIeIbAACGiG8AABgivgEAYIj4BgCAIeIbAACGiG8AABgivgEAYIj4BgCAIeIbAACGiG8A\nABgivgEAYIj4BgCAISeN76q6tKp+v6oeqaqHq+pXlvHzq+qeqnps+Xreyjq3VNXhqnq0qq5dGX9j\nVT24XPehqqqtuVsAALDzbOSZ7xeS/KPuvjLJNUlurqorkxxMcm93X5Hk3uX7LNcdSPL6JNcl+XBV\nnbNs6yNJ3pvkiuVy3SbeFwAA2NFOGt/d/WR3f21Z/sMk30hycZLrk9yx3OyOJDcsy9cnubO7n+/u\n7yQ5nOTqqrooyWu7+8vd3Uk+sbIOAACc9V7ROd9VtS/JTyf5SpILu/vJ5aqnkly4LF+c5Hsrqz2x\njF28LB87DgAAu8KG47uqfiLJ7yR5f3c/t3rd8kx2b9ZOVdVNVXWoqg49++yzm7VZAADYVhuK76r6\nsayF9ye7+3eX4aeXU0myfH1mGT+S5NKV1S9Zxo4sy8eO/4juvr2793f3/r179270vgAAwI62kb92\nUkk+muQb3f0bK1fdneTGZfnGJJ9dGT9QVedW1WVZe2PlfcspKs9V1TXLNt+9sg4AAJz19mzgNn8p\nyd9K8mBVPbCM/dMktyW5q6rek+S7Sd6ZJN39cFXdleSRrP2llJu7+8Vlvfcl+XiSVyX5wnIBAIBd\n4aTx3d3/Pcl6f4/7Leusc2uSW48zfijJG17JDgIAwNnCJ1wCAMAQ8Q0AAEPENwAADBHfAAAwRHwD\nAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHf\nAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPE\nNwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ\n8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAw\nRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAEPENwAADBHfAAAwRHwDAMAQ8Q0AAENOGt9V\n9bGqeqaqHloZ+2BVHamqB5bL21euu6WqDlfVo1V17cr4G6vqweW6D1VVbf7dAQCAnWsjz3x/PMl1\nxxn/19191XL5fJJU1ZVJDiR5/bLOh6vqnOX2H0ny3iRXLJfjbRMAAM5aJ43v7v5Skh9ucHvXJ7mz\nu5/v7u8kOZzk6qq6KMlru/vL3d1JPpHkhlPdaQAAOBOdzjnfv1xVX19OSzlvGbs4yfdWbvPEMnbx\nsnzsOAAA7BqnGt8fSXJ5kquSPJnk1zdtj5JU1U1VdaiqDj377LObuWkAANg2pxTf3f10d7/Y3X+U\n5LeSXL1cdSTJpSs3vWQZO7IsHzu+3vZv7+793b1/7969p7KLAACw45xSfC/ncB/1S0mO/iWUu5Mc\nqKpzq+qyrL2x8r7ufjLJc1V1zfJXTt6d5LOnsd8AAHDG2XOyG1TVp5K8OckFVfVEkl9N8uaquipJ\nJ3k8yd9Lku5+uKruSvJIkheS3NzdLy6bel/W/nLKq5J8YbkAAMCucdL47u53HWf4oye4/a1Jbj3O\n+KEkb3hFewcAAGcRn3AJAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPEN\nAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8\nAwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR\n3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABD\nxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMER8AwDA\nEPENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAw5aXxX1ceq6pmqemhl7PyquqeqHlu+nrdy\n3S1VdbiqHq2qa1fG31hVDy7XfaiqavPvDgAA7Fwbeeb740muO2bsYJJ7u/uKJPcu36eqrkxyIMnr\nl3U+XFXnLOt8JMl7k1yxXI7dJgAAnNVOGt/d/aUkPzxm+PokdyzLdyS5YWX8zu5+vru/k+Rwkqur\n6qIkr+3uL3d3J/nEyjoAALArnOo53xd295PL8lNJLlyWL07yvZXbPbGMXbwsHzsOAAC7xmm/4XJ5\nJrs3YV9eUlU3VdWhqjr07LPPbuamAQBg25xqfD+9nEqS5eszy/iRJJeu3O6SZezIsnzs+HF19+3d\nvb+79+/du/cUdxEAAHaWU43vu5PcuCzfmOSzK+MHqurcqrosa2+svG85ReW5qrpm+Ssn715ZBwAA\ndoU9J7tBVX0qyZuTXFBVTyT51SS3Jbmrqt6T5LtJ3pkk3f1wVd2V5JEkLyS5ubtfXDb1vqz95ZRX\nJfnCcgEAgF3jpPHd3e9a56q3rHP7W5PcepzxQ0ne8Ir2DgAAziI+4RIAAIaIbwAAGCK+AQBgiPgG\nAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+\nAQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaI\nbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh\n4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBg\niPgGAIAh4hsAAIbs2e4dOJvtO/i5Ldv247e9Y8u2DQDA1vDMNwAADBHfAAAwRHwDAMAQ8Q0AAEPE\nNwAADBHfAAAwRHwDAMAQ8Q0AAENOK76r6vGqerCqHqiqQ8vY+VV1T1U9tnw9b+X2t1TV4ap6tKqu\nPd2dBwCAM8lmPPP9l7v7qu7ev3x/MMm93X1FknuX71NVVyY5kOT1Sa5L8uGqOmcTfj4AAJwRtuK0\nk+uT3LEs35HkhpXxO7v7+e7+TpLDSa7egp8PAAA70unGdyf5YlXdX1U3LWMXdveTy/JTSS5cli9O\n8r2VdZ9YxgAAYFfYc5rrv6m7j1TVn0lyT1V9c/XK7u6q6le60SXkb0qS173udanT3EkAANgJTuuZ\n7+4+snx9JslnsnYaydNVdVGSLF+fWW5+JMmlK6tfsowdb7u3d/f+7t6/d+/e09lFAADYMU45vqvq\n1VX1mqPLSd6a5KEkdye5cbnZjUk+uyzfneRAVZ1bVZcluSLJfaf68wEA4ExzOqedXJjkM1V1dDu/\n3d3/uaq+muSuqnpPku8meWeSdPfDVXVXkkeSvJDk5u5+8bT2HgAAziCnHN/d/e0kf/444z9I8pZ1\n1rk1ya2n+jMBAOBM5hMuAQBgiPgGAIAh4hsAAIaIbwAAGCK+AQBgiPgGAIAh4hsAAIaIbwAAGCK+\nAQBgiPgGAIAh4hsAAIaIbwAAGLJnu3eAU7Pv4Oe2dPuP3/aOLd0+AMBu5JlvAAAYIr4BAGCI+AYA\ngCHiGwAAhohvAAAYIr4BAGCI+AYAgCHiGwAAhohvAAAYIr4BAGCI+AYAgCHiGwAAhohvAAAYIr4B\nAGCI+AYAgCHiGwAAhohvAAAYIr4BAGCI+AYAgCHiGwAAhohvAAAYIr4BAGCI+AYAgCHiGwAAhohv\nAAAYIr4BAGCI+AYAgCHiGwAAhohvAAAYIr4BAGCI+AYAgCHiGwAAhohvAAAYIr4BAGDInu3eAeDM\ntu/g57Zs24/f9o4t2zbztvJYSRwvwJnBM98AADDEM98cl2czAQA2n/gG4CVbfWoIwG7ntBMAABgi\nvgEAYIjTTgA4K3ivCnAmEN+M8w8kALBbOe0EAACGeOYbAE7CK3bAZhHfnFX8AwkA7GTj8V1V1yX5\nN0nOSfLvu/u26X0AAODlPIE1YzS+q+qcJP8uyV9J8kSSr1bV3d39yOR+wKk4kz98xIPe2eVMPhb5\nUWfyf0+PLT9qq/97mvMzX3X33A+r+otJPtjd1y7f35Ik3f0v1ltn//79/f2f/+dDewjsJFv9j8yZ\nHD1wttvK33+/+2eXzTpWqur+7t6/KRs7genTTi5O8r2V759I8heG9wE4Q/gHEnYvv/9s1Jl2rOzI\nN1xW1U1Jblq+fT73/8JD27k/Z5ELknx/u3fiLGI+N5f53Fzmc/OYy81lPjeX+dw8PzXxQ6bj+0iS\nS1e+v2QZe5nuvj3J7UlSVYcmXgLYDczl5jKfm8t8bi7zuXnM5eYyn5vLfG6eqjo08XOmP2Tnq0mu\nqKrLqurHkxxIcvfwPgAAwLYYfea7u1+oqn+Q5Pey9qcGP9bdD0/uAwAAbJfxc767+/NJPv8KVrl9\nq/ZlFzKXm8t8bi7zubnM5+Yxl5vLfG4u87l5RuZy9E8NAgDAbjZ9zjcAAOxaOza+q+q6qnq0qg5X\n1cHt3p+doqourarfr6pHqurhqvqVZfyDVXWkqh5YLm9fWeeWZR4fraprV8bfWFUPLtd9qKpqGT+3\nqj69jH+lqvZN389JVfX4Mg8PHH2nc1WdX1X3VNVjy9fzVm5vPtdRVT+1cgw+UFXPVdX7HZ8bU1Uf\nq6pnquqhlbGRY7Gqblx+xmNVdePMPd5a68znv6qqb1bV16vqM1X1p5bxfVX1f1aO0d9cWcd8Zt35\nHPnd3kXz+emVuXy8qh5Yxh2fJ1Drt9HOfPzs7h13ydqbMb+V5PIkP57kD5Jcud37tRMuSS5K8jPL\n8muS/I8kVyb5YJJ/fJzbX7nM37lJLlvm9ZzluvuSXJOkknwhyduW8fcl+c1l+UCST2/3/d7iOX08\nyQXHjP3LJAeX5YNJfs18vuJ5PSfJU0n+rONzw3P2c0l+JslDk8dikvOTfHv5et6yfN52z8cWzedb\nk+xZln9tZT73rd7umO2Yz/Xnc8t/t3fTfB5z/a8n+WeOzw3N5XpttCMfP3fqM99XJznc3d/u7v+b\n5M4k12/zPu0I3f1kd39tWf7DJN/I2ieHruf6JHd29/Pd/Z0kh5NcXVUXJXltd3+5146eTyS5YWWd\nO5bl/5TkLUf/z28XWZ2DO/LyuTGfG/OWJN/q7u+e4Dbmc0V3fynJD48ZnjgWr01yT3f/sLv/Z5J7\nkly3+fdw1vHms7v/S3e/sHz75ax93sS6zOcfW+f4XI/j8yRONJ/L/X5nkk+daBvmc80J2mhHPn7u\n1Pg+3sfQnygwd6XlJY+fTvKVZeiXa+2l1I+tvLSy3lxevCwfO/6ydZZ/pP5Xkj+9BXdhp+gkX6yq\n+2vt01WT5MLufnJZfirJhcuy+dy4A3n5PxyOz1MzcSzu1sfcv5u1Z7aOumx5Sf+/VdXPLmPm8+S2\n+nd7t81nkvxskqe7+7GVMcfnBhzTRjvy8XOnxjcnUVU/keR3kry/u59L8pGsnaZzVZIns/ZyFRvz\npu6+KsnbktxcVT+3euXyf7/+LNArUGsfovWLSf7jMuT43ASOxc1TVR9I8kKSTy5DTyZ53fJY8A+T\n/HZVvXa79u8M4nd7a7wrL3/ywvG5Acdpo5fspMfPnRrfG/oY+t2qqn4sawfXJ7v7d5Oku5/u7he7\n+4+S/FbWTt1J1p/LI3n5y62rc/zSOlW1J8lPJvnB1tyb7dfdR5avzyT5TNbm7unl5aejL+s9s9zc\nfG7M25J8rbufThyfp2niWNxVj7lV9beT/EKSv7n8g5zl5ecfLMv3Z+0c0D8X83lCQ7/bu2Y+k5fu\n+19L8umjY47PkzteG2WHPn7u1Pj2MfTrWM4v+miSb3T3b6yMX7Rys19KcvTd03cnObC8S/eyJFck\nuW95Gea5qrpm2ea7k3x2ZZ2j79b9G0n+69F/oM42VfXqqnrN0eWsvRnrobx8Dm7My+fGfJ7cy561\ncXyelolj8feSvLWqzltOG3jrMnbWqarrkvyTJL/Y3f97ZXxvVZ2zLF+etfn8tvk8saHf7V0zn4uf\nT/LN7n7p9AfH54mt10bZqY+fvQPepXq8S5K3Z+3dqt9K8oHt3p+dcknypqy9bPL1JA8sl7cn+Q9J\nHlzG705y0co6H1jm8dEs79pdxvdn7YHyW0n+bf74Q5f+ZNZOFzictXf9Xr7d93sL5/PyrL3j+Q+S\nPHz0WMvaeVz3JnksyReTnG8+Nzynr87aswE/uTLm+NzY3H0qay8v/7+snTf4nqljMWvnPx9eLn9n\nu+diC+fzcNbOzzz6+Hn0rxf89eUx4IEkX0vyV83nhuZz5Hd7t8znMv7xJH//mNs6Pk88l+u10Y58\n/PQJlwAAMGSnnnYCAABnHfENAABDxDcAAAwR3wAAMER8AwDAEPENAABDxDcAAAwR3wAAMOT/A9Ts\nUVvuSgpRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b0bb51c2190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=[12,6])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(maxlengths, bins=1000);\n",
    "ax.set_xlim(0,200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6810, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Truncateto the first 35000 words\n",
    "padded = pad_sequences(full_features, maxlen=5000, truncating='post')\n",
    "\n",
    "print padded.shape # Should be a num_samples by num_features np array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this all looks good, save the array for later\n",
    "\n",
    "np.save(\"../data/features/100_cutoff_alphabet_19679_padded_len5.npy\", padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padded = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In future, can load just with this line \n",
    "full_features = np.load(\"../data/features/100_cutoff_alphabet_19679_padded_len5.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3724, 5000)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# pull out training examples\n",
    "X = full_features[:classes.shape[0],:]\n",
    "\n",
    "X_test = full_features[classes.shape[0]:,:]\n",
    "print X_test.shape\n",
    "\n",
    "Y = classes_to_Y(classes)\n",
    "\n",
    "Y_hot = np.zeros((classes.shape[0], 16))\n",
    "for i, clazz in enumerate(Y):\n",
    "    Y_hot[i,clazz] = 1\n",
    "\n",
    "print Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "8\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Just to check that worked ok.\n",
    "print classes[21]\n",
    "print Y[21]\n",
    "print Y_hot[21]\n",
    "print len(malware_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2986, 5000)\n",
      "(2986, 16)\n",
      "(100, 5000)\n",
      "(100, 16)\n"
     ]
    }
   ],
   "source": [
    "# Now randomly select 100 samples to hold out\n",
    "rand_index = np.random.permutation(np.arange(classes.shape[0]))\n",
    "\n",
    "X_train = X[rand_index[100:]]\n",
    "Y_train = Y_hot[rand_index[100:]]\n",
    "X_validate = X[rand_index[:100]]\n",
    "Y_validate = Y_hot[rand_index[:100]]\n",
    "\n",
    "\n",
    "print X_train.shape\n",
    "\n",
    "print Y_train.shape\n",
    "\n",
    "print X_validate.shape\n",
    "\n",
    "print Y_validate.shape\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clobbering to save memory\n",
    "padding = 0\n",
    "full_features = 0\n",
    "classes= 0\n",
    "X = 0\n",
    "Y_hot = 0\n",
    "Y =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The vocabulary size is 2 + the maximum integer index. \n",
    "# To allow for padding (wout padding would be 1)\n",
    "vocab_size = 19681\n",
    "\n",
    "# Length of the dense embedding one for each int in the sequence\n",
    "embedding_length = 256 # arbitrary\n",
    "\n",
    "# Should be able to vary batch size with mask\n",
    "batch_size = 400\n",
    "model = Sequential()\n",
    "\n",
    "# Collapse the large input dimension into a 256 dimensional\n",
    "# dense embedding\n",
    "model.add(\n",
    "    Embedding(vocab_size, embedding_length, mask_zero=True)\n",
    ")\n",
    "\n",
    "# Could add a Dropout layer next but will avoid for now\n",
    "model.add(Bidirectional(\n",
    "    LSTM(100, return_sequences=True)\n",
    "))# Arbitrary output size. TODO make this stateful\n",
    "\n",
    "model.add(Dropout(.20)) # Regularize\n",
    "\n",
    "# Why not 2!\n",
    "model.add(LSTM(42)) # Arbitrary again\n",
    "model.add(Dropout(.20)) # Regularize\n",
    "\n",
    "model.add(Dense(200, activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ProgbarLogger, History, LambdaCallback, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "from __future__ import print_function\n",
    "summarize = lambda *__: print([psutil.virtual_memory(),psutil.cpu_percent(percpu=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2986 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "[svmem(total=270846246912, available=251037863936, percent=7.3, used=18839867392, free=175315251200, active=27105095680, inactive=54706941952, buffers=387674112, cached=76303454208, shared=103579648), [71.4, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.3, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0]]\n",
      "Epoch 1/5\n",
      "[svmem(total=270846246912, available=251037863936, percent=7.3, used=18839867392, free=175315251200, active=27105095680, inactive=54706941952, buffers=387674112, cached=76303454208, shared=103579648), [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      " 400/2986 [===>..........................] - ETA: 30518s - loss: 3.4535 - acc: 0.1025[svmem(total=270846246912, available=157231738880, percent=41.9, used=112646008832, free=81515630592, active=122807586816, inactive=52599881728, buffers=387674112, cached=76296933376, shared=103583744), [100.0, 0.1, 40.8, 0.0, 0.3, 0.0, 0.1, 0.0, 0.4, 0.2, 0.0, 0.2, 0.0, 0.1, 0.0, 0.1, 0.1, 0.0, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 1.0, 1.5, 0.0, 0.2, 0.0, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.2, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0]]\n",
      " 400/2986 [===>..........................] - ETA: 30518s - loss: 3.4535 - acc: 0.1025[svmem(total=270846246912, available=157231738880, percent=41.9, used=112646008832, free=81515630592, active=122807586816, inactive=52599881728, buffers=387674112, cached=76296933376, shared=103583744), [100.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      " 800/2986 [=======>......................] - ETA: 26023s - loss: 3.3444 - acc: 0.1100[svmem(total=270846246912, available=149085335552, percent=45.0, used=120792170496, free=78160384000, active=116267556864, inactive=62105960448, buffers=387608576, cached=71506083840, shared=103583744), [100.0, 0.1, 87.3, 0.1, 7.6, 0.1, 0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.2, 0.1, 0.1, 0.1, 0.0, 0.1, 0.1, 0.0, 1.0, 1.0, 0.2, 0.4, 0.0, 0.0, 0.3, 0.1, 0.1, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0]]\n",
      " 800/2986 [=======>......................] - ETA: 26023s - loss: 3.3444 - acc: 0.1100[svmem(total=270846246912, available=149085970432, percent=45.0, used=120792190976, free=78161018880, active=116267040768, inactive=62106476544, buffers=387608576, cached=71505428480, shared=103583744), [100.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "1200/2986 [===========>..................] - ETA: 20787s - loss: 3.2509 - acc: 0.1092[svmem(total=270846246912, available=160989749248, percent=40.6, used=108887973888, free=108987645952, active=105522651136, inactive=44534919168, buffers=387555328, cached=52583071744, shared=103583744), [100.0, 0.1, 49.5, 0.1, 10.3, 0.2, 0.4, 0.1, 0.4, 0.2, 0.1, 0.2, 0.1, 0.1, 0.0, 0.1, 0.3, 0.3, 0.0, 0.4, 0.1, 0.0, 0.0, 0.0, 0.9, 0.2, 0.0, 0.4, 0.1, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.6, 0.7, 0.1, 0.4, 0.1, 0.0, 0.1, 0.0, 0.5, 0.2, 0.0, 0.1, 0.2, 0.0, 0.0, 0.0, 0.2, 0.1, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0]]\n",
      "1200/2986 [===========>..................] - ETA: 20787s - loss: 3.2509 - acc: 0.1092[svmem(total=270846246912, available=160989749248, percent=40.6, used=108887973888, free=108987645952, active=105522651136, inactive=44534919168, buffers=387555328, cached=52583071744, shared=103583744), [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "1600/2986 [===============>..............] - ETA: 15458s - loss: 3.1535 - acc: 0.1125[svmem(total=270846246912, available=161441230848, percent=40.4, used=108436492288, free=116960698368, active=105951731712, inactive=37027500032, buffers=387551232, cached=45061505024, shared=103583744), [100.0, 0.1, 84.3, 0.1, 0.0, 0.0, 0.1, 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 1.1, 0.9, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.3, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0]]\n",
      "1600/2986 [===============>..............] - ETA: 15458s - loss: 3.1535 - acc: 0.1125[svmem(total=270846246912, available=161441230848, percent=40.4, used=108436492288, free=116960698368, active=105951731712, inactive=37027500032, buffers=387551232, cached=45061505024, shared=103583744), [100.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ProgbarLogger(), \n",
    "    History(),\n",
    "    LambdaCallback(\n",
    "        on_batch_begin=summarize, \n",
    "        on_batch_end=summarize, \n",
    "        on_epoch_begin=summarize),\n",
    "    ModelCheckpoint(\n",
    "        \"35small_mask_best.hdf5\", \n",
    "        verbose=1, \n",
    "        monitor=\"val_acc\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True)\n",
    "    ]\n",
    "\n",
    "model.fit(\n",
    "    X_train, Y_train, batch_size=batch_size,\n",
    "    nb_epoch=5, verbose=1, callbacks=callbacks, \n",
    "    validation_data=(X_validate, Y_validate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "class_preds = model.predict_classes(X_test)\n",
    "class_prob = model.predict_proba(X_test)\n",
    "np.save(\"../predictions/tiny_seq_LSTM.npy\", predictions)\n",
    "np.save(\"../predictions/tiny_seq_class_LSTM.npy\", class_preds)\n",
    "np.save(\"../predictions/tiny_seq_class_proba_LSTM.npy\", class_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
