{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the second attempt at training a NN. To handle variable sequence\n",
    "# lengths last time I had to feed examples one at a time, but this is\n",
    "# abhorently slow. I am trying another technique here, which is padding\n",
    "# The sequences and then using a mask internally to permit variable\n",
    "# Lengths. If this doesn't work I will just have to pad the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "malware_classes = [\"Agent\", \"AutoRun\", \"FraudLoad\", \"FraudPack\", \"Hupigon\", \"Krap\",\n",
    "           \"Lipler\", \"Magania\", \"None\", \"Poison\", \"Swizzor\", \"Tdss\",\n",
    "           \"VB\", \"Virut\", \"Zbot\"]\n",
    "\n",
    "# a function for writing predictions in the required format\n",
    "def write_predictions(predictions, ids, outfile):\n",
    "    \"\"\"\n",
    "    assumes len(predictions) == len(ids), and that predictions[i] is the\n",
    "    index of the predicted class with the malware_classes list above for\n",
    "    the executable corresponding to ids[i].\n",
    "    outfile will be overwritten\n",
    "    \"\"\"\n",
    "    with open(outfile,\"w+\") as f:\n",
    "        # write header\n",
    "        f.write(\"Id,Prediction\\n\")\n",
    "        for i, history_id in enumerate(ids):\n",
    "            f.write(\"%s,%d\\n\" % (history_id, predictions[i]))\n",
    "\n",
    "def classes_to_Y(classes):\n",
    "    output = []\n",
    "    for cls in classes:\n",
    "        output.append(malware_classes.index(cls))\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training classes\n",
    "classes = np.load(\"../data/features/train_classes.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  734   566     4     5  1150   558   740     4   785  5849    11     3\n",
      "     1   547    12    40   804  2049   946     1   947     1   811   113\n",
      "   805     5   803    68  3872   808  1827  2759   807  1591   809  1174\n",
      "   806   654  1590  1592   478   604  5653   495   844    11     3     1\n",
      "   547    10     4     6  1222    13  4798     7  1983    12    40    14\n",
      "    11     3     2     9   680     8    10     4     6   700    13   702\n",
      "     7   698    12   696    14    11     3     2     9   134     8    10\n",
      "     4     6   309    13   310     7   308    12   303    14    11     3\n",
      "     2   694   709     2   678   725     5   697   723   727    10     4\n",
      "     6   729    13   730     7   728    12    40    14    11     3     2\n",
      "     9   798     8    10     4     6   818    13   817     7   792    12\n",
      "   802    14    11     3     2     9   216     8    10     4     6   476\n",
      "    13   477     7   475    12   474    14    11     3     2     9   774\n",
      "     8    10     4     6   779    13   780     7   778    12   775    14\n",
      "    11     3     2     9   776     8    10     4     6   783    13   784\n",
      "     7   597    12   777    14    11     3     2     9   742     8    10\n",
      "     4     6   747    13   746     7   743    12   744    14    11     3\n",
      "     2     9   543     8    10     4     6   633    13   636     7   630\n",
      "    12   635    14    11     3     2     9   812     8    10     4     6\n",
      "   815    13   816     7   814    12   813    14    11     3     2     9\n",
      "   751     8    10     4     6   755    13   754     7   748    12   752\n",
      "    14    11     3     2     9   600     8    10     4     6   692    13\n",
      "   693     7   691    12   684    14    11     3     2     9   515     8\n",
      "    10     4     6   611    13   612     7   610    12   609    14    11\n",
      "     3     2     9   840     8    10     4     6   842    13   843     7\n",
      "   828    12   841    14    11     3     2     9   834     8    10     4\n",
      "     6   838    13   839     7   597    12   836  1104   145  1136    14\n",
      "    11     3     2     9   790     8    10     4     6   800    13   801\n",
      "     7   799    12   791   463    29    22    24    23    30   244   883\n",
      "     1   547    29    22    24    23    30   244   677    42    22    24\n",
      "    23    30   244   677    41   935   537   133   244  1265   618     4\n",
      "   291   152   532   549   551   509   468     5   537   133   244  1264\n",
      "   618     4   291   152   532   549   551   509   468     5   537   133\n",
      "   244  1258   618     4   291   152   532   549   551   509   468     5\n",
      "   537   133   244  1237   618     4   291   152   532   549   551   509\n",
      "   468     5   537   133   244  1263   618     4   291   152   532   549\n",
      "   551   509   468     5    29    22    64   624   576   638    42    22\n",
      "    64   624   576   638    41   845   749    42    22    64   624   576\n",
      "   638    41   576   749   463    29    22    24    23    30   244    42\n",
      "    22    24    23    30   244    41  1266   537   133   244  1260  1273\n",
      "     4   291   152   532   549   551  1261  1270     4   291   152   532\n",
      "   468     5   758   761  1132   555  5653   759  1168   760   757   758\n",
      "   761  1130   555  5653   759  1167   760   757    29    22    24    23\n",
      "    30     2   442    45  1103    42    22    24    23    30     2   442\n",
      "    45  1103    41   616    32    14    11     3     2     9   566     8\n",
      "    10     4     6   787    13   786     7   695    12   782   463    14\n",
      "    11     3     2     9   821   616    10     4     6 16495    13 16495\n",
      "     7     5    12   895   463    14    11     3     2     9   515     8\n",
      "    10     4     6   611    13   612     7   610    12   609    14    11\n",
      "     3     2     9   821   616    10     4     6  1095    13  1094     7\n",
      "   716    12   895   463   583   329 16568   115   616   171   590   616\n",
      "   580     5   579     5   575     5   578     5   489   756   585   613\n",
      "   589   586   587   584   588    29    22    64    23    30   244    42\n",
      "    22    64    23    30   244    41  1205   478  1230   632    29    22\n",
      "    24    23    30   244   677    42    22    24    23    30   244   677\n",
      "    41   935   463    14    11     3     2     9   821   616    10     4\n",
      "     6  1095    13  1094     7   716    12   895   583   329 17885   115\n",
      "   467   171  4342  4342  4342   580  5782   579 18236   575   108   578\n",
      "   354   489  1131  1152  1151   585   613  1577  1064   589   586 14554\n",
      "  1431  1625   587   584   588  1148    14    11     3     2     9   790\n",
      "     8    10     4     6   800    13   801     7   799    12   791    14\n",
      "    11     3     2     9   751     8    10     4     6   755    13   754\n",
      "     7   748    12   752   499    32   452   448   486    40   484  5378\n",
      "     1     1    41     1   499    32   452   448   486    40   484  5378\n",
      "     1     1    41     1   583   329  9611   115   821  1516   171   254\n",
      "   580     5   579     5   575     5   578     5   489   756   585   613\n",
      "   589   586   587   584   588   156   115  2209   256   583   329 11878\n",
      "   115   539   171 17247   580     5   579     5   575     5   578     5\n",
      "   489   756   585   613   589   586   587   584   588   614   329 17885\n",
      "  1251  1389     1   685   256   614   329 17885  1251  1389     1   685\n",
      "    29    22    64   624   576   638    42    22    64   624   576   638\n",
      "    41   845   749    42    22    64   624   576   638    41   576   749\n",
      "    29    22    64    23    30   244  1372    29    22    24    23    30\n",
      "   244  1372   537   133  1069   735  1640  6224   468     5   463   495\n",
      "   478   558  1150]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-42bf646f6aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# since this worked correctly, I can use keras to pad the sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Automatically pads up to longest seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m# Should be a num_samples by num_features np array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/scrb152/Software/Python/cs181/lib/python2.7/site-packages/keras/preprocessing/sequence.pyc\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/n/scrb152/Software/Python/cs181/lib/python2.7/site-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \"\"\"\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The way masking works, 0 must be maintained as a \"ignore this\" symbol,\n",
    "# so I have to increase the index by 1 to free the zero index.\n",
    "for i in xrange(len(full_features)):\n",
    "    full_features[i] +=1 #Add 1 to each of the arrays in the array\n",
    "    \n",
    "print full_features[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The way masking works, 0 must be maintained as a \"ignore this\" symbol,\n",
    "# so I have to increase the index by 1 to free the zero index.\n",
    "for i in xrange(len(full_features)):\n",
    "    full_features[i] +=1 #Add 1 to each of the arrays in the array\n",
    "    \n",
    "print full_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlengths = [len(x) for x in full_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1000000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAFpCAYAAADZfE25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b13fa92e890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=[12,6])\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(maxlengths, bins=200);\n",
    "ax.set_xlim(0,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6810, 200000)\n"
     ]
    }
   ],
   "source": [
    "# Looks like I will have to truncate the sequences in order to get anything\n",
    "# reasonable. There are some out to 6million, but it looks like almost all\n",
    "# could be captured at length 200000\n",
    "\n",
    "# Truncateto the first 200000 words\n",
    "padded = pad_sequences(full_features, maxlen=200000, truncating='post')\n",
    "\n",
    "print padded.shape # Should be a num_samples by num_features np array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If this all looks good, save the array for later\n",
    "\n",
    "np.save(\"../data/features/100_cutoff_alphabet_19679_padded_len200.npy\", padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "padded = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In future, can load just with this line \n",
    "full_features = np.load(\"../data/features/100_cutoff_alphabet_19679_padded_len200.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3724, 200000)\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  1.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# pull out training examples\n",
    "X = full_features[:classes.shape[0],:]\n",
    "\n",
    "X_test = full_features[classes.shape[0]:,:]\n",
    "print X_test.shape\n",
    "\n",
    "Y = classes_to_Y(classes)\n",
    "\n",
    "Y_hot = np.zeros((classes.shape[0], 16))\n",
    "for i, clazz in enumerate(Y):\n",
    "    Y_hot[i,clazz] = 1\n",
    "\n",
    "print Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "8\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Just to check that worked ok.\n",
    "print classes[21]\n",
    "print Y[21]\n",
    "print Y_hot[21]\n",
    "print len(malware_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2986, 200000)\n",
      "(2986, 16)\n",
      "(100, 200000)\n",
      "(100, 16)\n"
     ]
    }
   ],
   "source": [
    "# Now randomly select 100 samples to hold out\n",
    "rand_index = np.random.permutation(np.arange(classes.shape[0]))\n",
    "\n",
    "X_train = X[rand_index[100:]]\n",
    "Y_train = Y_hot[rand_index[100:]]\n",
    "X_validate = X[rand_index[:100]]\n",
    "Y_validate = Y_hot[rand_index[:100]]\n",
    "\n",
    "\n",
    "print X_train.shape\n",
    "\n",
    "print Y_train.shape\n",
    "\n",
    "print X_validate.shape\n",
    "\n",
    "print Y_validate.shape\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clobbering to save memory\n",
    "padding = 0\n",
    "full_features = 0\n",
    "classes= 0\n",
    "X = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The vocabulary size is 2 + the maximum integer index. \n",
    "# To allow for padding (wout padding would be 1)\n",
    "vocab_size = 19681\n",
    "\n",
    "# Length of the dense embedding one for each int in the sequence\n",
    "embedding_length = 256 # arbitrary\n",
    "\n",
    "# Should be able to vary batch size with mask\n",
    "batch_size = 80\n",
    "model = Sequential()\n",
    "\n",
    "# Collapse the large input dimension into a 256 dimensional\n",
    "# dense embedding\n",
    "model.add(\n",
    "    Embedding(vocab_size, embedding_length, mask_zero=True)\n",
    ")\n",
    "\n",
    "# Could add a Dropout layer next but will avoid for now\n",
    "model.add(Bidirectional(\n",
    "    LSTM(100, return_sequences=True)\n",
    "))# Arbitrary output size. TODO make this stateful\n",
    "\n",
    "# Why not 2!\n",
    "model.add(LSTM(42)) # Arbitrary again\n",
    "\n",
    "model.add(Dense(200, activation=\"sigmoid\"))\n",
    "model.add(Dense(16, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=\"adam\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ProgbarLogger, History, LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "from __future__ import print_function\n",
    "summarize = lambda *__: print([psutil.virtual_memory(),psutil.cpu_percent(percpu=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2986 samples, validate on 100 samples\n",
      "Epoch 1/5\n",
      "[svmem(total=270846246912, available=232580489216, percent=14.1, used=37297233920, free=161685471232, active=45634285568, inactive=51054436352, buffers=388141056, cached=71475400704, shared=103583744), [82.1, 0.2, 0.0, 0.1, 0.0, 0.1, 0.1, 0.0, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 3.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "Epoch 1/5\n",
      "[svmem(total=270846246912, available=232516435968, percent=14.2, used=37361287168, free=161621417984, active=45698281472, inactive=51054436352, buffers=388141056, cached=71475400704, shared=103583744), [100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ProgbarLogger(), \n",
    "    History(),\n",
    "    LambdaCallback(\n",
    "        on_batch_begin=summarize, \n",
    "        on_batch_end=summarize, \n",
    "        on_epoch_begin=summarize\n",
    "    )]\n",
    "\n",
    "model.fit(\n",
    "    X_train, Y_train, batch_size=batch_size,\n",
    "    nb_epoch=5, verbose=1, callbacks=callbacks, \n",
    "    validation_data=(X_validate, Y_validate)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"ok\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
