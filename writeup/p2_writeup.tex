\documentclass[11pt]{article}

\usepackage{common}
\title{Practical 2: Malware Classification}
\author{Ethan Alley, Grigory Khimulya, Walter Martin \\ alley@college.harvard.edu, khimulya@college.harvard.edu, wmartin@college.harvard.edu }
\begin{document}


\maketitle{}

\section{Technical Approach}

We explored the problem of malware classification from several different angles:

\begin{itemize}
	\item Feature engineering
	\item Running through a variety of models quickly to test which show the most immediate promise
	\item More focused tuning of hyperparameters for neural nets
\end{itemize}

\noindent Feature engineering process (Grigory and Ethan)
\linebreak

\noindent With several different feature sets to test on, we tested out classification with a random forest, a support vector machine, a standard neural net (sklearn's MLPClassifier), and an LSTM net.
\linebreak

\noindent Tuning standard neural net parameters was an accessory step, more with the intention of learning about overfitting rather than improving performance. The parameters tuned were:
	
	\begin{itemize}
		\item Number of hidden layers (1 or 2)
		\item Size of hidden layers (between 800 and 50)
		\item Activation function (tanh, relu, or logistic)
		\item Maximum number of optimization iterations
	\end{itemize}

\pagebreak

How did you tackle the problem? Credit will be given for:

  \begin{itemize}
  \item Diving deeply into a method (rather than just trying
    off-the-shelf tools with default settings). This can mean 
    providing mathematical descriptions or pseudo-code.
  \item Making tuning and configuration decisions using thoughtful experimentation.  
    This can mean carefully describing features added or hyperparameters tuned.
  \item Exploring several methods. This can contrasting two approaches
    or perhaps going beyond those we discussed in class.
  \end{itemize}

  \noindent Thoughtfully iterating on approaches is key.
  If you used existing packages or referred to papers or blogs for ideas,
  you should cite these in your report. 



\section{Results}
We had a wide variety of performance across the methods and feature sets we tested. For the models that we submitted to Kaggle, our best result was approximately $.79$ accuracy on the private tests, produced by a random forest classifier.
 \\
\begin{table}
\centering
\begin{tabular}{llr}
	\toprule
	Model &  & Acc. \\
	\midrule
	\textsc{RF, $n=100$, on TFIDF} & & 0.791\\
	\textsc{Deep net on TFIDF} & & 0.785\\
	\textsc{SVM on 4G BOW} & & 0.772 \\
	\textsc{SVM on GBOW} & & 0.735  \\
	\textsc{SVM on topic modeled FV} & & 0.342 \\
	\textsc{Passive Aggressive on GBOW} & & 0.150\\
	\bottomrule
\end{tabular}
\caption{Model accuracy on private tests.}
\end{table}

In this table, GBOW is "garbage bag of words," FV is "feature vector," 4G is "4-gram," and TFIDF is [Ethan help!]

We also did some tuning of neural net parameters. These cross-validation scores were generated using sklearn's "cross val score" function; we took the average of 5-fold CV results. We were using the "tanh" activation unless otherwise specified. The numbers associated with the model is the dimension of each hidden layer. This data came from training on our length 10000 feature vectorization.

\begin{table}
	\centering
	\begin{tabular}{llr}
		\toprule
		Model &  & Acc. \\
		\midrule
		\textsc{400} & & 0.867 \\
		\textsc{400, logistic} & & 0.867 \\
		\textsc{400, 50, logistic} & & 0.867  \\
		\textsc{400, 100} & & 0.866 \\
		\textsc{800} & & 0.866 \\
		\textsc{400, 50} & & 0.863  \\
		\textsc{400, 200} & & 0.862 \\
		\textsc{200} & & 0.862 \\
		\textsc{200, 100} & & 0.860 \\
		\textsc{100} & & 0.858 \\
		\textsc{400, relu} & & 0.855 \\
		\bottomrule
	\end{tabular}
	\caption{Neural network 5-fold CV accuracy.}
\end{table}

\pagebreak

This section should report on the following questions: 

\begin{itemize}
\item Did you create and submit a set of predictions? 
  

\item  Did your methods give reasonable performance?  
\end{itemize}

\noindent You must have \textit{at least one plot or table}
that details the performances of different methods tried. 
Credit will be given for quantitatively reporting (with clearly
labeled and captioned figures and/or tables) on the performance of the
methods you tried compared to your baselines.



\section{Discussion} 


End your report by discussing the thought process behind your
analysis. This section does not need to be as technical as the others 
but should summarize why you took the approach that your did. Credit will be given for:

  \begin{itemize}
  \item Explaining the your reasoning for why you seqentially chose to
    try the approaches you did (i.e. what was it about your initial
    approach that made you try the next change?).  
  \item Explaining the results.  Did the adaptations you tried improve
    the results?  Why or why not?  Did you do additional tests to
    determine if your reasoning was correct?  
  \end{itemize}
 

\end{document}

