\documentclass[11pt]{article}

\usepackage{common}
\title{Practical 2: Malware Classification}
\author{Ethan Alley, Grigory Khimulya, Walter Martin \\ alley@college.harvard.edu, khimulya@college.harvard.edu, wmartin@college.harvard.edu }
\begin{document}


\maketitle{}

\section{Technical Approach}

We explored the problem of malware classification from several different angles:

\begin{itemize}
	\item Feature engineering
	\item Running through a variety of models quickly to test which show the most immediate promise
	\item More focused tuning of hyperparameters for neural nets
\end{itemize}

\noindent Feature engineering process (Grigory and Ethan)
\linebreak

\noindent With several different feature sets to test on, we tested out classification with a random forest, a support vector machine, a standard neural net (sklearn's MLPClassifier), and an LSTM net.
\linebreak

\noindent Tuning standard neural net parameters was an accessory step, more with the intention of learning about overfitting rather than improving performance. The parameters tuned were:
	
	\begin{itemize}
		\item Number of hidden layers (1 or 2)
		\item Size of hidden layers (between 800 and 50)
		\item Activation function (tanh, relu, or logistic)
		\item Maximum number of optimization iterations
	\end{itemize}

\pagebreak


\section{Results}
We had a wide variety of performance across the methods and feature sets we tested. For the models that we submitted to Kaggle, our best result was approximately $.79$ accuracy on the private tests, produced by a random forest classifier.
 \\
\begin{table}[h]
\centering
\begin{tabular}{llr}
	\toprule
	Model &  & Acc. \\
	\midrule
	\textsc{RF, $n=100$, on TFIDF} & & 0.791\\
	\textsc{Deep net on TFIDF} & & 0.785\\
	\textsc{SVM on 4G BOW} & & 0.772 \\
	\textsc{SVM on GBOW} & & 0.735  \\
	\textsc{SVM on topic modeled FV} & & 0.342 \\
	\textsc{Passive Aggressive on GBOW} & & 0.150\\
	\bottomrule
\end{tabular}
\caption{Model accuracy on private tests.}
\end{table}

In this table, GBOW is "garbage bag of words," FV is "feature vector," 4G is "4-gram," and TFIDF is [Ethan help!]

We also did some tuning of neural net parameters. These cross-validation scores were generated using sklearn's "cross val score" function; we took the average of 5-fold CV results. We were using the "tanh" activation unless otherwise specified. The numbers associated with the model is the dimension of each hidden layer. This data came from training on our length 10000 feature vectorization.

We found that a 1-layer neural net with a size $400$ hidden layer performed equally as well or better than more complex methods with the tanh activation, and as it took less training time than the more complex models, we used this architecture to test different activation functions. We found that logistic and tanh activations had similar classification performance, but logistic took longer to train. ReLU didn't match up so well. We also tried a higher maximum number of training iterations on the highest performing architecture, but the results were unchanged.

\begin{table}[h]
	\centering
	\begin{tabular}{llr}
		\toprule
		Model &  & Acc. \\
		\midrule
		\textsc{400} & & 0.867 \\
		\textsc{400, logistic} & & 0.867 \\
		\textsc{400, 50, logistic} & & 0.867  \\
		\textsc{400, 100} & & 0.866 \\
		\textsc{800} & & 0.866 \\
		\textsc{400, 50} & & 0.863  \\
		\textsc{400, 200} & & 0.862 \\
		\textsc{200} & & 0.862 \\
		\textsc{200, 100} & & 0.860 \\
		\textsc{100} & & 0.858 \\
		\textsc{400, relu} & & 0.855 \\
		\bottomrule
	\end{tabular}
	\caption{Neural network 5-fold CV accuracy.}
\end{table}

We also tried a very simple boosting method for less common classes, where we just copied each example of the least representative classes. It improved our cross-validation accuracy to $.872$, but generalized poorly, indicating overfitting.


\section{Discussion} 

Our first set of features had relatively good performance, but was clunky to work with, as it was a very high-dimensional sparse matrix. This drove the development of smaller feature sets ($m = 10^4$ rather than $10^6$), which made it much easier to iterate on models quickly. Indeed, we weren't even able to run some classifiers on a standard Mac laptop with the first feature set.

For model evaluation, we began with a baseline of an SVM with straightforward bag-of-words features and stochastic gradient descent optimization.

From there, we tried a Passive Aggressive classifier (Ethan, maybe you can elaborate a bit more?) which didn't do very well.

Iterating on the basic SVM, we were able to improve with a new version using the Huber loss function. We got a significant (about $2.5$ percent) accuracy improvement from this switch. The Huber loss function is likely an improvement because..... not sure.

Even training on a 4-gram feature set, though, the SVM seemed to be hitting a limit, so from there we tried an LSTM neural net and a random forest classifier, which both led to slight improvements.

The fact that we were held around $80$ percent accuracy regardless of some reasonable varied model choices most likely means that if we wanted to make substantial gains, we'd need to do more feature work, like with more domain-specific knowledge or at least educated guessing. 


\end{document}

